{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1vcRyT0thov",
        "outputId": "0d62ff3f-c042-475a-ad80-68af76802667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Unigram Counts ===\n",
            "       <s> : 3\n",
            "      </s> : 3\n",
            "         I : 2\n",
            "      love : 2\n",
            "      deep : 2\n",
            "  learning : 2\n",
            "       NLP : 1\n",
            "        is : 1\n",
            "       fun : 1\n",
            "\n",
            "=== Bigram Counts ===\n",
            "( <s>, I       ) : 2\n",
            "(   I, love    ) : 2\n",
            "(deep, learning) : 2\n",
            "(love, NLP     ) : 1\n",
            "( NLP, </s>    ) : 1\n",
            "(love, deep    ) : 1\n",
            "(learning, </s>    ) : 1\n",
            "( <s>, deep    ) : 1\n",
            "(learning, is      ) : 1\n",
            "(  is, fun     ) : 1\n",
            "( fun, </s>    ) : 1\n",
            "\n",
            "Sentence: <s> I love NLP </s>\n",
            "  P(I | <s>) = count(<s>,I)/count(<s>) = 2/3 = 0.666667\n",
            "  P(love | I) = count(I,love)/count(I) = 2/2 = 1.000000\n",
            "  P(NLP | love) = count(love,NLP)/count(love) = 1/2 = 0.500000\n",
            "  P(</s> | NLP) = count(NLP,</s>)/count(NLP) = 1/1 = 1.000000\n",
            "\n",
            "Sentence: <s> I love deep learning </s>\n",
            "  P(I | <s>) = count(<s>,I)/count(<s>) = 2/3 = 0.666667\n",
            "  P(love | I) = count(I,love)/count(I) = 2/2 = 1.000000\n",
            "  P(deep | love) = count(love,deep)/count(love) = 1/2 = 0.500000\n",
            "  P(learning | deep) = count(deep,learning)/count(deep) = 2/2 = 1.000000\n",
            "  P(</s> | learning) = count(learning,</s>)/count(learning) = 1/2 = 0.500000\n",
            "\n",
            "=== Sentence Probabilities (Bigram MLE, no smoothing) ===\n",
            "P(s1) = P(<s> I love NLP </s>) = 0.3333333333\n",
            "P(s2) = P(<s> I love deep learning </s>) = 0.1666666667\n",
            "\n",
            "=== Model Preference ===\n",
            "The model prefers s1.\n",
            "Reason: because its product of bigram MLE probabilities is larger.\n",
            "\n",
            "Extra explanation:\n",
            "Both sentences share the same prefix bigrams: (<s>, I) and (I, love).\n",
            "So the preference is determined by the remaining bigrams in each sentence.\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Training corpus (given)\n",
        "# ----------------------------\n",
        "TRAINING_SENTENCES = [\n",
        "    \"<s> I love NLP </s>\",\n",
        "    \"<s> I love deep learning </s>\",\n",
        "    \"<s> deep learning is fun </s>\",\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def tokenize(sentence: str) -> List[str]:\n",
        "    # Simple whitespace tokenization (works for this corpus)\n",
        "    return sentence.strip().split()\n",
        "\n",
        "def get_bigrams(tokens: List[str]) -> List[Tuple[str, str]]:\n",
        "    return list(zip(tokens[:-1], tokens[1:]))\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Compute unigram & bigram counts\n",
        "# ----------------------------\n",
        "def build_counts(corpus: List[str]) -> Tuple[Counter, Counter]:\n",
        "    unigram_counts = Counter()\n",
        "    bigram_counts = Counter()\n",
        "\n",
        "    for sent in corpus:\n",
        "        tokens = tokenize(sent)\n",
        "        unigram_counts.update(tokens)\n",
        "        bigram_counts.update(get_bigrams(tokens))\n",
        "\n",
        "    return unigram_counts, bigram_counts\n",
        "\n",
        "# ----------------------------\n",
        "# 3) MLE bigram probabilities\n",
        "#    P(w2 | w1) = count(w1,w2) / count(w1)\n",
        "# ----------------------------\n",
        "def mle_bigram_prob(w1: str, w2: str, unigram_counts: Counter, bigram_counts: Counter) -> float:\n",
        "    c_bigram = bigram_counts[(w1, w2)]\n",
        "    c_unigram = unigram_counts[w1]\n",
        "    if c_unigram == 0:\n",
        "        return 0.0\n",
        "    return c_bigram / c_unigram\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Sentence probability under bigram model\n",
        "#    P(w1..wn) = Î  P(wi | w(i-1))\n",
        "#    (No smoothing: any unseen bigram => prob = 0)\n",
        "# ----------------------------\n",
        "def sentence_probability(sentence: str, unigram_counts: Counter, bigram_counts: Counter) -> float:\n",
        "    tokens = tokenize(sentence)\n",
        "    prob = 1.0\n",
        "    for w1, w2 in get_bigrams(tokens):\n",
        "        p = mle_bigram_prob(w1, w2, unigram_counts, bigram_counts)\n",
        "        prob *= p\n",
        "    return prob\n",
        "\n",
        "def explain_sentence(sentence: str, unigram_counts: Counter, bigram_counts: Counter) -> None:\n",
        "    tokens = tokenize(sentence)\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    for w1, w2 in get_bigrams(tokens):\n",
        "        c = bigram_counts[(w1, w2)]\n",
        "        p = mle_bigram_prob(w1, w2, unigram_counts, bigram_counts)\n",
        "        print(f\"  P({w2} | {w1}) = count({w1},{w2})/count({w1}) = {c}/{unigram_counts[w1]} = {p:.6f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Main\n",
        "# ----------------------------\n",
        "def main():\n",
        "    unigram_counts, bigram_counts = build_counts(TRAINING_SENTENCES)\n",
        "\n",
        "    print(\"=== Unigram Counts ===\")\n",
        "    for tok, c in unigram_counts.most_common():\n",
        "        print(f\"{tok:>10} : {c}\")\n",
        "\n",
        "    print(\"\\n=== Bigram Counts ===\")\n",
        "    for (w1, w2), c in bigram_counts.most_common():\n",
        "        print(f\"({w1:>4}, {w2:<8}) : {c}\")\n",
        "\n",
        "    # Test sentences (given)\n",
        "    s1 = \"<s> I love NLP </s>\"\n",
        "    s2 = \"<s> I love deep learning </s>\"\n",
        "\n",
        "    # Show factor probabilities\n",
        "    explain_sentence(s1, unigram_counts, bigram_counts)\n",
        "    explain_sentence(s2, unigram_counts, bigram_counts)\n",
        "\n",
        "    p1 = sentence_probability(s1, unigram_counts, bigram_counts)\n",
        "    p2 = sentence_probability(s2, unigram_counts, bigram_counts)\n",
        "\n",
        "    print(\"\\n=== Sentence Probabilities (Bigram MLE, no smoothing) ===\")\n",
        "    print(f\"P(s1) = P({s1}) = {p1:.10f}\")\n",
        "    print(f\"P(s2) = P({s2}) = {p2:.10f}\")\n",
        "\n",
        "    # Preference\n",
        "    if p1 > p2:\n",
        "        preferred = \"s1\"\n",
        "        why = \"because its product of bigram MLE probabilities is larger.\"\n",
        "    elif p2 > p1:\n",
        "        preferred = \"s2\"\n",
        "        why = \"because its product of bigram MLE probabilities is larger.\"\n",
        "    else:\n",
        "        preferred = \"tie\"\n",
        "        why = \"because both sentences have the same probability under this model.\"\n",
        "\n",
        "    print(\"\\n=== Model Preference ===\")\n",
        "    if preferred == \"tie\":\n",
        "        print(\"The model is indifferent (tie).\")\n",
        "    else:\n",
        "        print(f\"The model prefers {preferred}.\")\n",
        "    print(f\"Reason: {why}\")\n",
        "\n",
        "    # A concrete \"why\" for this corpus:\n",
        "    # In this corpus, the shared prefix bigrams are identical:\n",
        "    # (<s>, I), (I, love)\n",
        "    # The difference comes from the tail.\n",
        "    print(\"\\nExtra explanation:\")\n",
        "    print(\"Both sentences share the same prefix bigrams: (<s>, I) and (I, love).\")\n",
        "    print(\"So the preference is determined by the remaining bigrams in each sentence.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}